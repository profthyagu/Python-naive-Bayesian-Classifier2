{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Problem: Assuming a set of documents that need to be classified, use the naïve Bayesian Classifier model to perform this task. Built-in Libraries can be used to write the program. Calculate the accuracy, precision, and recall for your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the 20 newsgroups dataset : The dataset is called “Twenty Newsgroups”. Here is the official description, quoted from the website:http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The number of categories: 20\n",
      "\n",
      " The 20 Different Categories of 20Newsgroups\n",
      "\n",
      "Category[1]: alt.atheism\n",
      "Category[2]: comp.graphics\n",
      "Category[3]: comp.os.ms-windows.misc\n",
      "Category[4]: comp.sys.ibm.pc.hardware\n",
      "Category[5]: comp.sys.mac.hardware\n",
      "Category[6]: comp.windows.x\n",
      "Category[7]: misc.forsale\n",
      "Category[8]: rec.autos\n",
      "Category[9]: rec.motorcycles\n",
      "Category[10]: rec.sport.baseball\n",
      "Category[11]: rec.sport.hockey\n",
      "Category[12]: sci.crypt\n",
      "Category[13]: sci.electronics\n",
      "Category[14]: sci.med\n",
      "Category[15]: sci.space\n",
      "Category[16]: soc.religion.christian\n",
      "Category[17]: talk.politics.guns\n",
      "Category[18]: talk.politics.mideast\n",
      "Category[19]: talk.politics.misc\n",
      "Category[20]: talk.religion.misc\n",
      "\n",
      " Length of training data is 11314\n",
      "\n",
      " Length of file names is  11314\n",
      "\n",
      " The Content/Data of First File is :\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author : Dr.Thyagaraju G S , Context Innovations Lab , DEpt of CSE , SDMIT - Ujire \n",
    "# Date : July 11 2018 \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "x = len(twenty_train.target_names)\n",
    "print(\"\\n The number of categories:\",x)\n",
    "print(\"\\n The %d Different Categories of 20Newsgroups\\n\" %x)\n",
    "i=1\n",
    "for cat in twenty_train.target_names:\n",
    "    print(\"Category[%d]:\" %i,cat)\n",
    "    i=i+1\n",
    "print(\"\\n Length of training data is\",len(twenty_train.data))\n",
    "print(\"\\n Length of file names is \",len(twenty_train.filenames))\n",
    "\n",
    "print(\"\\n The Content/Data of First File is :\\n\")\n",
    "\n",
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Contents/Data of First 10 Files is in Training Data :\n",
      "\n",
      "\n",
      " FILE NO:1 \n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " FILE NO:2 \n",
      "\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "\n",
      " FILE NO:3 \n",
      "\n",
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n",
      "\n",
      " FILE NO:4 \n",
      "\n",
      "From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      "\n",
      "\n",
      " FILE NO:5 \n",
      "\n",
      "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\n",
      "Subject: Re: Shuttle Launch Question\n",
      "Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\n",
      "Distribution: sci\n",
      "Lines: 23\n",
      "\n",
      "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      ">>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n",
      ">>>\"Clear caution & warning memory.  Verify no unexpected\n",
      ">>>errors. ...\".  I am wondering what an \"expected error\" might\n",
      ">>>be.  Sorry if this is a really dumb question, but\n",
      "> \n",
      "> Parity errors in memory or previously known conditions that were waivered.\n",
      ">    \"Yes that is an error, but we already knew about it\"\n",
      "> I'd be curious as to what the real meaning of the quote is.\n",
      "> \n",
      "> tom\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
      "\n",
      " - Jonathan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " FILE NO:6 \n",
      "\n",
      "From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\n",
      "Subject: Re: Rewording the Second Amendment (ideas)\n",
      "Organization: VTT\n",
      "Lines: 58\n",
      "\n",
      "In article <1r1eu1$4t@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
      ">In article <1993Apr20.083057.16899@ousrvr.oulu.fi>, dfo@vttoulu.tko.vtt.fi (Foxvog Douglas) writes:\n",
      ">> In article <1qv87v$4j3@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
      ">> >In article <C5n3GI.F8F@ulowell.ulowell.edu>, jrutledg@cs.ulowell.edu (John Lawrence Rutledge) writes:\n",
      ">\n",
      ">> >> The massive destructive power of many modern weapons, makes the\n",
      ">> >> cost of an accidental or crimial usage of these weapons to great.\n",
      ">> >> The weapons of mass destruction need to be in the control of\n",
      ">> >> the government only.  Individual access would result in the\n",
      ">> >> needless deaths of millions.  This makes the right of the people\n",
      ">> >> to keep and bear many modern weapons non-existant.\n",
      "\n",
      ">> >Thanks for stating where you're coming from.  Needless to say, I\n",
      ">> >disagree on every count.\n",
      "\n",
      ">> You believe that individuals should have the right to own weapons of\n",
      ">> mass destruction?  I find it hard to believe that you would support a \n",
      ">> neighbor's right to keep nuclear weapons, biological weapons, and nerve\n",
      ">> gas on his/her property.  \n",
      "\n",
      ">> If we cannot even agree on keeping weapons of mass destruction out of\n",
      ">> the hands of individuals, can there be any hope for us?\n",
      "\n",
      ">I don't sign any blank checks.\n",
      "\n",
      "Of course.  The term must be rigidly defined in any bill.\n",
      "\n",
      ">When Doug Foxvog says \"weapons of mass destruction,\" he means CBW and\n",
      ">nukes.  When Sarah Brady says \"weapons of mass destruction\" she means\n",
      ">Street Sweeper shotguns and semi-automatic SKS rifles.  \n",
      "\n",
      "I doubt she uses this term for that.  You are using a quote allegedly\n",
      "from her, can you back it up?\n",
      "\n",
      ">When John\n",
      ">Lawrence Rutledge says \"weapons of mass destruction,\" and then immediately\n",
      ">follows it with:\n",
      "\n",
      ">>> The US has thousands of people killed each year by handguns,\n",
      ">>> this number can easily be reduced by putting reasonable restrictions\n",
      ">>> on them.\n",
      "\n",
      ">...what does Rutledge mean by the term?\n",
      "\n",
      "I read the article as presenting first an argument about weapons of mass\n",
      "destruction (as commonly understood) and then switching to other topics.\n",
      "The first point evidently was to show that not all weapons should be\n",
      "allowed, and then the later analysis was, given this understanding, to\n",
      "consider another class.\n",
      "\n",
      ">cdt@rocket.sw.stratus.com   --If you believe that I speak for my company,\n",
      ">OR cdt@vos.stratus.com        write today for my special Investors' Packet...\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "doug foxvog\n",
      "douglas.foxvog@vtt.fi\n",
      "\n",
      "\n",
      " FILE NO:7 \n",
      "\n",
      "From: bmdelane@quads.uchicago.edu (brian manning delaney)\n",
      "Subject: Brain Tumor Treatment (thanks)\n",
      "Reply-To: bmdelane@midway.uchicago.edu\n",
      "Organization: University of Chicago\n",
      "Lines: 12\n",
      "\n",
      "There were a few people who responded to my request for info on\n",
      "treatment for astrocytomas through email, whom I couldn't thank\n",
      "directly because of mail-bouncing probs (Sean, Debra, and Sharon).  So\n",
      "I thought I'd publicly thank everyone.\n",
      "\n",
      "Thanks! \n",
      "\n",
      "(I'm sure glad I accidentally hit \"rn\" instead of \"rm\" when I was\n",
      "trying to delete a file last September. \"Hmmm... 'News?' What's\n",
      "this?\"....)\n",
      "\n",
      "-Brian\n",
      "\n",
      "\n",
      " FILE NO:8 \n",
      "\n",
      "From: bgrubb@dante.nmsu.edu (GRUBB)\n",
      "Subject: Re: IDE vs SCSI\n",
      "Organization: New Mexico State University, Las Cruces, NM\n",
      "Lines: 44\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: dante.nmsu.edu\n",
      "\n",
      "DXB132@psuvm.psu.edu writes:\n",
      ">In article <1qlbrlINN7rk@dns1.NMSU.Edu>, bgrubb@dante.nmsu.edu (GRUBB) says:\n",
      ">>In PC Magazine April 27, 1993:29 \"Although SCSI is twice as fasst as ESDI,\n",
      ">>20% faster than IDE, and support up to 7 devices its acceptance ...has   \n",
      ">>long been stalled by incompatability problems and installation headaches.\"\n",
      "                                                                      \n",
      ">I love it when magazine writers make stupid statements like that re:      \n",
      ">performance. Where do they get those numbers? I'll list the actual\n",
      ">performance ranges, which should convince anyone that such a               \n",
      ">statement is absurd:                                                     \n",
      ">SCSI-I ranges from 0-5MB/s.                                                \n",
      ">SCSI-II ranges from 0-40MB/s.            \n",
      ">IDE ranges from 0-8.3MB/s.                          \n",
      ">ESDI is always 1.25MB/s (although there are some non-standard versions)\n",
      "ALL this shows is that YOU don't know much about SCSI.\n",
      "\n",
      "SCSI-1 {with a SCSI-1 controler chip} range is indeed 0-5MB/s\n",
      "and that is ALL you have right about SCSI\n",
      "SCSI-1 {With a SCSI-2 controller chip}: 4-6MB/s with 10MB/s burst {8-bit}\n",
      " Note the INCREASE in SPEED, the Mac Quadra uses this version of SCSI-1\n",
      " so it DOES exist. Some PC use this set up too.\n",
      "SCSI-2 {8-bit/SCSI-1 mode}:          4-6MB/s with 10MB/s burst\n",
      "SCSI-2 {16-bit/wide or fast mode}:  8-12MB/s with 20MB/s burst\n",
      "SCSI-2 {32-bit/wide AND fast}:     15-20MB/s with 40MB/s burst\n",
      " \n",
      "By your OWN data the \"Although SCSI is twice as fast as ESDI\" is correct\n",
      "With a SCSI-2 controller chip SCSI-1 can reach 10MB/s which is indeed\n",
      "\"20% faster than IDE\" {120% of 8.3 is 9.96}. ALL these SCSI facts have been\n",
      "posted to this newsgroup in my Mac & IBM info sheet {available by FTP on \n",
      "sumex-aim.stanford.edu (36.44.0.6) in the info-mac/report as \n",
      "mac-ibm-compare[version #].txt (It should be 173 but 161 may still be there)}\n",
      "\n",
      "Part of this problem is both Mac and IBM PC are inconsiant about what SCSI\n",
      "is which.  Though it is WELL documented that the Quadra has a SCSI-2 chip\n",
      "an Apple salesperson said \"it uses a fast SCSI-1 chip\" {Not at a 6MB/s,\n",
      "10MB/s burst it does not. SCSI-1 is 5MB/s maximum synchronous and Quadra\n",
      "uses ANsynchronous SCSI which is SLOWER}  It seems that Mac and IBM see\n",
      "SCSI-1 interface and think 'SCSI-1' when it maybe a SCSI-1 interface driven\n",
      "in the machine by a SCSi-2 controller chip in 8-bit mode {Which is MUCH\n",
      "FASTER then true SCSI-1 can go}.\n",
      "\n",
      "Don't slam an article because you don't understand what is going on.\n",
      "One reference for the Quadra's SCSI-2 controller chip is \n",
      "(Digital Review, Oct 21, 1991 v8 n33 p8(1)).\n",
      "\n",
      "\n",
      " FILE NO:9 \n",
      "\n",
      "From: holmes7000@iscsvax.uni.edu\n",
      "Subject: WIn 3.0 ICON HELP PLEASE!\n",
      "Organization: University of Northern Iowa\n",
      "Lines: 10\n",
      "\n",
      "I have win 3.0 and downloaded several icons and BMP's but I can't figure out\n",
      "how to change the \"wallpaper\" or use the icons.  Any help would be appreciated.\n",
      "\n",
      "\n",
      "Thanx,\n",
      "\n",
      "-Brando\n",
      "\n",
      "PS Please E-mail me\n",
      "\n",
      "\n",
      "\n",
      " FILE NO:10 \n",
      "\n",
      "From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\n",
      "Subject: Re: Sigma Designs Double up??\n",
      "Article-I.D.: ux1.C52u8x.B62\n",
      "Organization: University of Illinois at Urbana\n",
      "Lines: 29\n",
      "\n",
      "jap10@po.CWRU.Edu (Joseph A. Pellettiere) writes:\n",
      "\n",
      "\n",
      ">\tI am looking for any information about the Sigma Designs\n",
      ">\tdouble up board.  All I can figure out is that it is a\n",
      ">\thardware compression board that works with AutoDoubler, but\n",
      ">\tI am not sure about this.  Also how much would one cost?\n",
      "\n",
      "I've had the board for over a year, and it does work with Diskdoubler,\n",
      "but not with Autodoubler, due to a licensing problem with Stac Technologies,\n",
      "the owners of the board's compression technology. (I'm writing this\n",
      "from memory; I've lost the reference. Please correct me if I'm wrong.)\n",
      "\n",
      "Using the board, I've had problems with file icons being lost, but it's\n",
      "hard to say whether it's the board's fault or something else; however,\n",
      "if I decompress the troubled file and recompress it without the board,\n",
      "the icon usually reappears. Because of the above mentioned licensing\n",
      "problem, the freeware expansion utility DD Expand will not decompress\n",
      "a board-compressed file unless you have the board installed.\n",
      "\n",
      "Since Stac has its own product now, it seems unlikely that the holes\n",
      "in Autodoubler/Diskdoubler related to the board will be fixed.\n",
      "Which is sad, and makes me very reluctant to buy Stac's product since\n",
      "they're being so stinky. (But hey, that's competition.)\n",
      "-- \n",
      "\n",
      "Stan Kerr    \n",
      "Computing & Communications Services Office, U of Illinois/Urbana\n",
      "Phone: 217-333-5217  Email: stankerr@uiuc.edu   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n The Contents/Data of First 10 Files is in Training Data :\\n\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"\\n FILE NO:%d \\n\"%(i+1))\n",
    "    print(twenty_train.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering only four Categories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reduced Target Names:\n",
      " ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "\n",
      " Reduced Target Length:\n",
      " 2257\n",
      "\n",
      "First Document :  From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',     categories=categories, shuffle=True, random_state=42)\n",
    "print(\"\\n Reduced Target Names:\\n\",twenty_train.target_names)\n",
    "print(\"\\n Reduced Target Length:\\n\", len(twenty_train.data))\n",
    "print(\"\\nFirst Document : \",twenty_train.data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.The most intuitive way to do so is the bags of words representation:\n",
    "assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "for each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary\n",
    "The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "Tokenizing text with scikit-learn:\n",
    "Text preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a dictionary of features and transform documents to feature vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Occurrences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Target Length , Distinct Words): (2257, 35788)\n",
      "\n",
      " Frequency of the word algorithm: 4690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print(\"\\n(Target Length , Distinct Words):\",X_train_counts.shape) \n",
    "print(\"\\n Frequency of the word algorithm:\", count_vect.vocabulary_.get('algorithm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From occurrences to frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequencies : Divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”. Both tf and tf–idf can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example-code, we firstly use the fit(..) method to fit our estimator to the data and secondly the transform(..) method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the fit_transform(..) method as shown below, and as mentioned in the note in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Outcome"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, since they have already been fit to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The names vect, tfidf and clf (classifier) are arbitrary. We shall see their use in the section on grid search, below. We can now train the model with a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the predictive accuracy of the model is equally easy:\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scikit-learn further provides utilities for more detailed performance analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "     target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As expected the confusion matrix shows that posts from the newsgroups on atheism and christian are more often confused for one another than with computer graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reference : http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
